{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119c9460",
   "metadata": {},
   "source": [
    "## Home assignment 05: Bagging and OOB score\n",
    "\n",
    "Please, fill the lines in the code below.\n",
    "This is a simplified version of `BaggingRegressor` from `sklearn`. Please, notice, that `sklearn` API is **not preserved**.\n",
    "\n",
    "Your algorithm should be able to train different instances of the same model class on bootstrapped datasets and to provide [OOB score](https://en.wikipedia.org/wiki/Out-of-bag_error) for the training set.\n",
    "\n",
    "The model should be passed as model class with no explicit parameters and no parentheses.\n",
    "\n",
    "Example:\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "bagging_regressor = SimplifiedBaggingRegressor(num_bags=10, oob=True)\n",
    "bagging_regressor.fit(LinearRegression, X, y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ecde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06110580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBaggingRegressor:\n",
    "    def __init__(self, num_bags, oob=False):\n",
    "        self.num_bags = num_bags\n",
    "        self.oob = oob\n",
    "        \n",
    "    def _generate_splits(self, data: np.ndarray):\n",
    "        '''\n",
    "        Generate indices for every bag and store in self.indices_list list\n",
    "        '''\n",
    "        self.indices_list = []\n",
    "        data_length = len(data)\n",
    "        for bag in range(self.num_bags):\n",
    "            bag_indices = np.random.randint(0, data_length, data_length)\n",
    "            self.indices_list.append(bag_indices)\n",
    "        \n",
    "    def fit(self, model_constructor, data, target):\n",
    "        '''\n",
    "        Fit model on every bag.\n",
    "        Model constructor with no parameters (and with no ()) is passed to this function.\n",
    "        \n",
    "        example:\n",
    "        \n",
    "        bagging_regressor = SimplifiedBaggingRegressor(num_bags=10, oob=True)\n",
    "        bagging_regressor.fit(LinearRegression, X, y)\n",
    "        '''\n",
    "        self.data = None\n",
    "        self.target = None\n",
    "        self._generate_splits(data)\n",
    "        assert len(set(list(map(len, self.indices_list)))) == 1, 'All bags should be of the same length!'\n",
    "        assert list(map(len, self.indices_list))[0] == len(data), 'All bags should contain `len(data)` number of elements!'\n",
    "        self.models_list = []\n",
    "        for bag in range(self.num_bags):\n",
    "            model = model_constructor()\n",
    "            data_bag = data[self.indices_list[bag]]\n",
    "            target_bag = target[self.indices_list[bag]]\n",
    "            self.models_list.append(model.fit(data_bag, target_bag)) # store fitted models here\n",
    "        if self.oob:\n",
    "            self.data = data\n",
    "            self.target = target\n",
    "        \n",
    "    def predict(self, data):\n",
    "        '''\n",
    "        Get average prediction for every object from passed dataset\n",
    "        '''\n",
    "        predictions = np.zeros(len(data))\n",
    "        for model in self.models_list:\n",
    "            predictions += model.predict(data)\n",
    "        return predictions / self.num_bags\n",
    "    \n",
    "    def _get_oob_predictions_from_every_model(self):\n",
    "        '''\n",
    "        Generates list of lists, where list i contains predictions for self.data[i] object\n",
    "        from all models, which have not seen this object during training phase\n",
    "        '''\n",
    "        list_of_predictions_lists = [[] for _ in range(len(self.data))]\n",
    "        for idx in range(len(self.data)):\n",
    "            sample = self.data[idx].reshape(-1, 1)\n",
    "            models_predictions = []\n",
    "            for bag in range(self.num_bags):\n",
    "                if idx not in self.indices_list[bag]:\n",
    "                    models_predictions.append(float(self.models_list[bag].predict(sample)))\n",
    "            list_of_predictions_lists[idx] = models_predictions\n",
    "        self.list_of_predictions_lists = np.array(list_of_predictions_lists, dtype=object)\n",
    "    \n",
    "    def _get_averaged_oob_predictions(self):\n",
    "        '''\n",
    "        Compute average prediction for every object from training set.\n",
    "        If object has been used in all bags on training phase, return None instead of prediction\n",
    "        '''\n",
    "        self._get_oob_predictions_from_every_model()\n",
    "        self.oob_predictions = np.array([np.mean(predictions) if len(predictions) > 0 else np.nan for predictions in self.list_of_predictions_lists])\n",
    "        \n",
    "        \n",
    "    def OOB_score(self):\n",
    "        '''\n",
    "        Compute mean square error for all objects, which have at least one prediction\n",
    "        '''\n",
    "        self._get_averaged_oob_predictions()\n",
    "        return np.nanmean((self.oob_predictions - self.target)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa174f",
   "metadata": {},
   "source": [
    "### Local tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa2e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54221c2",
   "metadata": {},
   "source": [
    "#### Simple tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84c94a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b0458da9e14d04ac6b48b9fae9542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1 features, but LinearRegression is expecting 10 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean((predictions \u001b[38;5;241m-\u001b[39m y)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear dependency should be fitted with almost zero error!\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m bagging_regressor\u001b[38;5;241m.\u001b[39moob, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOOB feature must be turned on\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m oob_score \u001b[38;5;241m=\u001b[39m \u001b[43mbagging_regressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOOB_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m oob_score \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOOB error for linear dependency should be also close to zero!\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(\n\u001b[0;32m     12\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, bagging_regressor\u001b[38;5;241m.\u001b[39mlist_of_predictions_lists))\n\u001b[0;32m     14\u001b[0m     ) \u001b[38;5;241m/\u001b[39m bagging_regressor\u001b[38;5;241m.\u001b[39mnum_bags \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProbability of missing a bag should be close to theoretical value!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn [29], line 78\u001b[0m, in \u001b[0;36mSimplifiedBaggingRegressor.OOB_score\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mOOB_score\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    Compute mean square error for all objects, which have at least one prediction\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_averaged_oob_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnanmean((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moob_predictions \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn [29], line 70\u001b[0m, in \u001b[0;36mSimplifiedBaggingRegressor._get_averaged_oob_predictions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_averaged_oob_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Compute average prediction for every object from training set.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    If object has been used in all bags on training phase, return None instead of prediction\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_oob_predictions_from_every_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moob_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mmean(predictions) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean \u001b[38;5;28;01mfor\u001b[39;00m predictions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_of_predictions_list])\n",
      "Cell \u001b[1;32mIn [29], line 61\u001b[0m, in \u001b[0;36mSimplifiedBaggingRegressor._get_oob_predictions_from_every_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_bags):\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_list[bag]:\n\u001b[1;32m---> 61\u001b[0m             models_predictions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbag\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     62\u001b[0m     list_of_predictions_lists[idx] \u001b[38;5;241m=\u001b[39m models_predictions\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_of_predictions_lists \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(list_of_predictions_lists, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:354\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:337\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    335\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 337\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1 features, but LinearRegression is expecting 10 features as input."
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    X = np.random.randn(2000, 10)\n",
    "    y = np.mean(X, axis=1)\n",
    "    bagging_regressor = SimplifiedBaggingRegressor(num_bags=10, oob=True)\n",
    "    bagging_regressor.fit(LinearRegression, X, y)\n",
    "    predictions = bagging_regressor.predict(X)\n",
    "    assert np.mean((predictions - y)**2) < 1e-6, 'Linear dependency should be fitted with almost zero error!'\n",
    "    assert bagging_regressor.oob, 'OOB feature must be turned on'\n",
    "    oob_score = bagging_regressor.OOB_score()\n",
    "    assert oob_score < 1e-6, 'OOB error for linear dependency should be also close to zero!'\n",
    "    assert abs(\n",
    "        np.mean(\n",
    "            list(map(len, bagging_regressor.list_of_predictions_lists))\n",
    "        ) / bagging_regressor.num_bags - 1/np.exp(1)) < 0.1, 'Probability of missing a bag should be close to theoretical value!'\n",
    "    \n",
    "print('Simple tests done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4d037",
   "metadata": {},
   "source": [
    "#### Medium tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    X = np.random.randn(200, 150)\n",
    "    y = np.random.randn(len(X))\n",
    "    bagging_regressor = SimplifiedBaggingRegressor(num_bags=20, oob=True)\n",
    "    bagging_regressor.fit(LinearRegression, X, y)\n",
    "    predictions = bagging_regressor.predict(X)\n",
    "    average_train_error = np.mean((predictions - y)**2)\n",
    "    assert bagging_regressor.oob, 'OOB feature must be turned on'\n",
    "    oob_score = bagging_regressor.OOB_score()\n",
    "    assert oob_score > average_train_error, 'OOB error must be higher than train error due to overfitting!'\n",
    "    assert abs(\n",
    "        np.mean(\n",
    "            list(map(len, bagging_regressor.list_of_predictions_lists))\n",
    "        ) / bagging_regressor.num_bags - 1/np.exp(1)) < 0.1, 'Probability of missing a bag should be close to theoretical value!'\n",
    "    \n",
    "print('Medium tests done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725818ff",
   "metadata": {},
   "source": [
    "#### Complex tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f929d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    X = np.random.randn(2000, 15)\n",
    "    y = np.random.randn(len(X))\n",
    "    bagging_regressor = SimplifiedBaggingRegressor(num_bags=100, oob=True)\n",
    "    bagging_regressor.fit(LinearRegression, X, y)\n",
    "    predictions = bagging_regressor.predict(X)\n",
    "    oob_score = bagging_regressor.OOB_score()\n",
    "    assert abs(\n",
    "        np.mean(\n",
    "            list(map(len, bagging_regressor.list_of_predictions_lists))\n",
    "        ) / bagging_regressor.num_bags - 1/np.exp(1)) < 1e-2, 'Probability of missing a bag should be close to theoretical value!'\n",
    "    \n",
    "print('Complex tests done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af170ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(\n",
    "            list(map(len, bagging_regressor.list_of_predictions_lists))\n",
    "        ) / bagging_regressor.num_bags - 1/np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535cb6d",
   "metadata": {},
   "source": [
    "Great job! Please, save `SimplifiedBaggingRegressor` to  `bagging.py` and submit your solution to the grading system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
